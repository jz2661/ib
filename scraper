import logging,os
import pandas as pd
import numpy as np
from datetime import datetime
from util import *

from linkedin_jobs_scraper import LinkedinScraper
from linkedin_jobs_scraper.events import Events, EventData, EventMetrics
from linkedin_jobs_scraper.query import Query, QueryOptions, QueryFilters
from linkedin_jobs_scraper.filters import RelevanceFilters, TimeFilters, TypeFilters, ExperienceLevelFilters, RemoteFilters

# Change root logger level (default is WARN)
logging.basicConfig(level = logging.WARN)

os.environ['LI_AT_COOKIE'] = r'AQEDAQ7-cxkFtGe8AAABglkj3I0AAAGCfTBgjVYAJL4aurh9YlOjhSAyQiKUf3UE-YfetnoFOnZ0zkHU1pgOfPy46h-8dFWzbDdpafxgg3I24YmVqYsrZbnERmdt-5CNvlFAuIuTOBwvIGAJY6rc4p_h'
os.chdir(r'C:\repo\linkedin')

joblist = []

# Fired once for each successfully processed job
def on_data(data: EventData):
    print('[ON_DATA]', data.title, data.company, data.company_link, data.date, data.link, data.insights, len(data.description))
    joblist.append(data)

# Fired once for each page (25 jobs)
def on_metrics(metrics: EventMetrics):
  print('[ON_METRICS]', str(metrics))

def on_error(error):
    print('[ON_ERROR]', error)

def on_end():
    print('[ON_END]')


scraper = LinkedinScraper(
    chrome_executable_path=None, # Custom Chrome executable path (e.g. /foo/bar/bin/chromedriver) 
    chrome_options=None,  # Custom Chrome options here
    headless=True,  # Overrides headless mode only if chrome_options is None
    max_workers=1,  # How many threads will be spawned to run queries concurrently (one Chrome driver for each thread)
    slow_mo=1.6,  # Slow down the scraper to avoid 'Too many requests 429' errors (in seconds)
    page_load_timeout=20  # Page load timeout (in seconds)    
)

# Add event listeners
scraper.on(Events.DATA, on_data)
scraper.on(Events.ERROR, on_error)
scraper.on(Events.END, on_end)

timefilter = TimeFilters.DAY

titles = ['Quantitative','Derivative','Python','Option Trader','Market Making','Vice President']
queries = [
    Query(
        query=title,
        options=QueryOptions(
            locations=['Hong Kong'],
            apply_link = True,  # Try to extract apply link (easy applies are skipped). Default to False.
            limit=200,
            filters=QueryFilters(              
                #company_jobs_url='https://www.linkedin.com/jobs/search/?f_C=1441%2C17876832%2C791962%2C2374003%2C18950635%2C16140%2C10440912&geoId=92000000',  # Filter by companies.
                relevance=RelevanceFilters.RELEVANT,
                time=timefilter,
                type=[TypeFilters.FULL_TIME],
                experience=None,                
            )
        )
    ) for title in titles
]

scraper.run(queries)

df = pd.DataFrame([expand_data(d) for d in joblist],columns=['date','title','company','ap','link','des','place'])

df = black(df)
df = rank(df)
df = remove_seen(df)
df.to_excel('new.xlsx')

send_mail(files=['new.xlsx'])

#largecompany = df.groupby('company').count().sort_values('date',ascending=False)[:30]    

j0 = joblist[0]

j0